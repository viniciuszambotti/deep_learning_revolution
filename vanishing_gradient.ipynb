{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import datetime as dt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, activation, num_layers= 6, hidden_size= 10):\n",
    "        # cria os layers do modelo\n",
    "        self.num_layers = num_layers\n",
    "        self.nn_model = tf.keras.Sequential()\n",
    "        \n",
    "        # cria 6 camadas densas com 10 neurônios cada\n",
    "        for i in range(num_layers):\n",
    "            self.nn_model.add(tf.keras.layers.Dense(hidden_size, activation= activation, name=f\"layer{i + 1}\"))\n",
    "        \n",
    "        # cria camada de saída\n",
    "        self.nn_model.add(tf.keras.layers.Dense(10, name=\"output_layer\"))\n",
    "    \n",
    "    @tf.function\n",
    "    def foward(self, input_images):\n",
    "        input_images = tf.cast(input_images, tf.float32)\n",
    "        input_images = tf.reshape(input_images, [-1, 28*28])\n",
    "        input_images = input_images/ 255.0\n",
    "        logits = self.nn_model(input_images)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    @staticmethod\n",
    "    def loss(logits, labels):\n",
    "        return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits= logits, labels = labels))\n",
    "    \n",
    "    def log_gradient(self, gradients, train_writer, step):\n",
    "        \n",
    "        # trainable_variables retorna as variaveis de treinamento, os pesos e o bias\n",
    "        assert len(gradients) == len(self.nn_model.trainable_variables)\n",
    "        \n",
    "        for i in range(len(gradients)):\n",
    "            # apenas acessa os dvalores de peso\n",
    "            if \"kernel\" in self.nn_model.trainable_variables[i].name:\n",
    "                \n",
    "                # Cria visualização, 1- valor absoluto dos gradientes, 2- histograma com os valores do gradiente, 3- valores dos pesos, para cada layer\n",
    "                with train_writer.as_default():\n",
    "                    tf.summary.scalar(f\"mean_{int((i-1)/2)}\", tf.reduce_mean(tf.abs(gradients[i])), step= step)\n",
    "                    tf.summary.histogram(f\"mean_{int((i-1)/2)}\", gradients[i], step= step)\n",
    "                    tf.summary.histogram(f\"mean_{int((i-1)/2)}\", self.nn_model.trainable_variables[i], step= step)\n",
    "        \n",
    "    \n",
    "    def plot_computational_graphs(self, train_writer, x_batch):\n",
    "        tf.summary.trace_on(graph= True)\n",
    "        self.foward(x_batch)\n",
    "        \n",
    "        with train_writer.as_default():\n",
    "            tf.summary.trace_export(name='graph', step= 0)\n",
    "\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, size):\n",
    "    idxs = np.random.randint(0, len(y), size)\n",
    "    return x[idxs,:,:], y[idxs]\n",
    "\n",
    "def run_training(model: Model, sub_folder: str, iterations: int= 2500, batch_size:int= 32, log_freq:int= 200):\n",
    "    dt_n = dt.datetime.now().strftime('%d%m%Y%H%M')\n",
    "    train_writer = tf.summary.create_file_writer(f\"tf_vis/vis_{dt_n}\")\n",
    "    model.plot_computational_graphs(train_writer, x_train[:batch_size, :, :])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # busca um batch aleatório de imagens\n",
    "        image_batch, label_batch = get_batch(x_train, y_train, batch_size)\n",
    "        \n",
    "        # transforma x e y em um tensor\n",
    "        image_batch = tf.constant(image_batch)\n",
    "        label_batch = tf.cast(tf.constant(label_batch), tf.int32)\n",
    "        \n",
    "        # captura operações feitas no feed-foward e na função de loss\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model.foward(image_batch)\n",
    "            loss = model.loss(logits, label_batch)\n",
    "        \n",
    "        #obtém os gradientes\n",
    "        gradients = tape.gradient(loss, model.nn_model.trainable_variables)\n",
    "        \n",
    "        # aplica o back-propagation\n",
    "        optimizer.apply_gradients(zip(gradients, model.nn_model.trainable_variables))\n",
    "        \n",
    "        if i % log_freq == 0:\n",
    "            max_idxs = tf.argmax(logits, axis= 1)\n",
    "            acc = np.sum(max_idxs.numpy() == label_batch.numpy()) / len(label_batch.numpy())\n",
    "            \n",
    "            print(f\"Iter: {i}, loss={loss:.3f}, accuracy={acc * 100:.3f}%\")\n",
    "            \n",
    "            with train_writer.as_default():\n",
    "                tf.summary.scalar('loss', loss, step=i)\n",
    "                tf.summary.scalar('accuracy', acc, step=i)\n",
    "            model.log_gradient(gradients, train_writer, i)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, loss=2.441, accuracy=18.750%\n",
      "Iter: 200, loss=2.314, accuracy=6.250%\n",
      "Iter: 400, loss=2.273, accuracy=15.625%\n",
      "Iter: 600, loss=2.210, accuracy=9.375%\n",
      "Iter: 800, loss=1.937, accuracy=18.750%\n",
      "Iter: 1000, loss=1.808, accuracy=18.750%\n",
      "Iter: 1200, loss=1.734, accuracy=46.875%\n",
      "Iter: 1400, loss=1.817, accuracy=31.250%\n",
      "Iter: 1600, loss=1.681, accuracy=31.250%\n",
      "Iter: 1800, loss=1.764, accuracy=25.000%\n",
      "Iter: 2000, loss=1.627, accuracy=34.375%\n",
      "Iter: 2200, loss=1.489, accuracy=31.250%\n",
      "Iter: 2400, loss=1.509, accuracy=34.375%\n",
      "Iter: 0, loss=2.293, accuracy=15.625%\n",
      "Iter: 200, loss=1.494, accuracy=50.000%\n",
      "Iter: 400, loss=0.793, accuracy=75.000%\n",
      "Iter: 600, loss=0.682, accuracy=81.250%\n",
      "Iter: 800, loss=0.366, accuracy=90.625%\n",
      "Iter: 1000, loss=0.597, accuracy=87.500%\n",
      "Iter: 1200, loss=0.471, accuracy=84.375%\n",
      "Iter: 1400, loss=0.770, accuracy=78.125%\n",
      "Iter: 1600, loss=0.401, accuracy=93.750%\n",
      "Iter: 1800, loss=0.352, accuracy=90.625%\n",
      "Iter: 2000, loss=0.218, accuracy=96.875%\n",
      "Iter: 2200, loss=0.457, accuracy=90.625%\n",
      "Iter: 2400, loss=0.446, accuracy=81.250%\n",
      "Iter: 0, loss=2.299, accuracy=15.625%\n",
      "Iter: 200, loss=1.551, accuracy=37.500%\n",
      "Iter: 400, loss=1.016, accuracy=65.625%\n",
      "Iter: 600, loss=0.706, accuracy=78.125%\n",
      "Iter: 800, loss=0.259, accuracy=93.750%\n",
      "Iter: 1000, loss=0.429, accuracy=87.500%\n",
      "Iter: 1200, loss=0.528, accuracy=78.125%\n",
      "Iter: 1400, loss=0.336, accuracy=93.750%\n",
      "Iter: 1600, loss=0.412, accuracy=81.250%\n",
      "Iter: 1800, loss=0.545, accuracy=87.500%\n",
      "Iter: 2000, loss=0.201, accuracy=93.750%\n",
      "Iter: 2200, loss=0.465, accuracy=90.625%\n",
      "Iter: 2400, loss=0.375, accuracy=90.625%\n"
     ]
    }
   ],
   "source": [
    "scenarios = ['sigmoid', \"relu\", \"leaky_relu\"]\n",
    "act_funcs = [tf.sigmoid, tf.nn.relu, tf.nn.leaky_relu]\n",
    "\n",
    "assert len(scenarios) == len(act_funcs)\n",
    "\n",
    "for i in range(len(scenarios)):\n",
    "    model = Model(act_funcs[i], 6, 10)\n",
    "    run_training(model, scenarios[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf tf_vis/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4",
   "language": "python",
   "name": "other-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
