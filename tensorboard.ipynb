{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define arquivo para visualização no tensorboard\n",
    "out_file = f\"tf_vis/hist_{dt.datetime.now().strftime('%d%m%Y%H%M')}\"\n",
    "train_summary_writer = tf.summary.create_file_writer(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def nn_model(x, labels, W1, b1, W2, b2):\n",
    "    x =  tf.reshape(x, (x.shape[0], -1))\n",
    "    \n",
    "    # cria namespace com as operações abaixo no tensorboard\n",
    "    with tf.name_scope(\"Hidden\") as scope:\n",
    "        hidden_logits = tf.add(tf.matmul(tf.cast(x, tf.float32), W1), b1)\n",
    "        hidden_out = tf.nn.sigmoid(hidden_logits)\n",
    "\n",
    "    with tf.name_scope(\"Output\") as scope:\n",
    "        logits = tf.add(tf.matmul(hidden_out, W2), b2)\n",
    "    \n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels= labels, logits= logits))\n",
    "    \n",
    "    return logits, hidden_logits, hidden_out, cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, size):\n",
    "    idxs = np.random.randint(0, len(y), size)\n",
    "    return x[idxs,:,:], y[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "epochs = 10\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# normaliza as imagens, valores ficam entre 0 e 1\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# converte o x_test para tensor para ser usado no modelo, dados de treinamento serão convertidos on the fly\n",
    "x_test = tf.Variable(x_test)\n",
    "\n",
    "# Declara pesos conectando o input com a camada oculta\n",
    "W1 = tf.Variable(tf.random.normal([784, 300], stddev= 0.03), name='w1')\n",
    "b1 = tf.Variable(tf.random.normal([300]), name='b1')\n",
    "\n",
    "# e os pesos conectando a camada oculta à camada de saida\n",
    "W2 = tf.Variable(tf.random.normal([300, 10], stddev= 0.03), name='w2')\n",
    "b2 = tf.Variable(tf.random.normal([10]), name='b2')\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cria gráfico com as operações feitas pela rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "# obtem uma amostra aleatória\n",
    "batch_x, batch_y = get_batch(x_train, y_train, batch_size)\n",
    "\n",
    "# Cria tensores\n",
    "batch_x = tf.constant(batch_x)\n",
    "batch_y = tf.constant(batch_y)\n",
    "\n",
    "# Cria vetor one hot\n",
    "batch_y = tf.one_hot(batch_y, 10)\n",
    "\n",
    "# Habilita o tensorboard a capturar informações\n",
    "tf.summary.trace_on(graph=True, profiler=True)\n",
    "\n",
    "logits = nn_model(batch_x, batch_y, W1, b1, W2, b2)\n",
    "with train_summary_writer.as_default():\n",
    "    tf.summary.trace_export(name=\"nn_model\",step=0, profiler_outdir=out_file)\n",
    "    \n",
    "print(\"\\nTraining complete\")            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/tb_puro.png)\n",
    "![title](images/tb_namespace.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cria grafico de loss, logits e das imagens preditas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, loss=0.551, test set    accuracy=92.320%\n",
      "Epoch:2, loss=0.226, test set    accuracy=94.250%\n",
      "Epoch:3, loss=0.173, test set    accuracy=95.240%\n",
      "Epoch:4, loss=0.134, test set    accuracy=96.000%\n",
      "Epoch:5, loss=0.113, test set    accuracy=96.630%\n",
      "Epoch:6, loss=0.095, test set    accuracy=96.820%\n",
      "Epoch:7, loss=0.075, test set    accuracy=97.100%\n",
      "Epoch:8, loss=0.063, test set    accuracy=97.290%\n",
      "Epoch:9, loss=0.056, test set    accuracy=97.580%\n",
      "Epoch:10, loss=0.047, test set    accuracy=97.750%\n",
      "\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "# determina o numero de batches para roda em cada época, garante que na média cada amostra de treinamento será usada\n",
    "total_batch = int(len(y_train)/ batch_size)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # acompanha a média de loss para cada época\n",
    "    avg_loss = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        \n",
    "        # obtem uma amostra aleatória\n",
    "        batch_x, batch_y = get_batch(x_train, y_train, batch_size)\n",
    "        \n",
    "        # Cria tensores\n",
    "        batch_x = tf.constant(batch_x)\n",
    "        batch_y = tf.constant(batch_y)\n",
    "        \n",
    "        # Cria vetor one hot\n",
    "        batch_y = tf.one_hot(batch_y, 10)\n",
    "        \n",
    "        # API para o TF calcular os gradients, qualquer operação e/ou variveis dentro desse contexto, o TF irá capturar os gradientes\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            # TF agora sabe quais operações e variaveis deve fazer o tracking, para  depois podermos fazer operações com os gradientes\n",
    "            logits, hidden_logits, hidden_out, loss = nn_model(batch_x, batch_y, W1, b1, W2, b2)\n",
    "         \n",
    "        #calcula a derivada (dL/dw & Dl/db)\n",
    "        gradients = tape.gradient(loss, [W1, b1, W2, b2])\n",
    "        \n",
    "        # associa o gradient aos pesos e bias e executa o gradiente descendente\n",
    "        optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2]))\n",
    "        \n",
    "        # Loss médio da época\n",
    "        avg_loss += loss / total_batch\n",
    "        \n",
    "    test_logits,_,_,_ = nn_model(x_test, tf.one_hot(y_test, 10), W1, b1, W2, b2)\n",
    "    max_idxs = tf.argmax(test_logits, axis= 1)\n",
    "    test_acc = np.sum(max_idxs.numpy() == y_test)/ len(y_test)\n",
    "    \n",
    "    print(f\"Epoch:{epoch + 1}, loss={avg_loss:.3f}, test set    accuracy={test_acc * 100:.3f}%\")\n",
    "    \n",
    "    if epoch == 0:\n",
    "        \n",
    "        # captura as imagens que o modelo acertou e error\n",
    "        correct_inputs = tf.boolean_mask(x_test, max_idxs.numpy()== y_test)\n",
    "        incorrect_inputs = tf.boolean_mask(x_test, tf.logical_not(max_idxs.numpy() == y_test))\n",
    "        \n",
    "        with train_summary_writer.as_default():\n",
    "            # restaura a imagem para o formato original de 28x28 e adiciona um limite de 5 amostras para mostrar no TB\n",
    "            tf.summary.image(\"correct images\", tf.reshape(correct_inputs, (-1, 28, 28, 1)), max_outputs=5, step=epoch)\n",
    "            tf.summary.image(\"incorrect images\", tf.reshape(incorrect_inputs, (-1, 28, 28, 1)), max_outputs=5, step=epoch)\n",
    "    \n",
    "    # cria visualização no TB com loss ,accuracy e os logists\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar(\"loss\", avg_loss, step=epoch)\n",
    "        tf.summary.scalar(\"accuracy\", test_acc, step=epoch)\n",
    "        tf.summary.histogram(\"Hidden_logits\", hidden_logits, step=epoch)\n",
    "        tf.summary.histogram(\"Hidden_out\", hidden_out, step=epoch)\n",
    "\n",
    "print(\"\\nTraining complete\")\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/acc.png)\n",
    "![title](images/logits.png)\n",
    "![title](images/imgs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4",
   "language": "python",
   "name": "other-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
