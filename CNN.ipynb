{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/CNN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, activation, input_channels, output_channels, window_size, pool_size, filt_stride, pool_stride,\n",
    "        initializer=tf.keras.initializers.he_normal()):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.initializer = initializer\n",
    "        self.activation = activation\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.window_size = window_size\n",
    "        self.pool_size = pool_size\n",
    "        self.filt_stride = filt_stride\n",
    "        self.pool_stride = pool_stride\n",
    "        self.w = self.add_weight(shape=(window_size[0], window_size[1], input_channels, output_channels),\n",
    "                                 initializer=self.initializer,trainable=True)\n",
    "        \n",
    "        self.b = self.add_weight(shape=(output_channels,), initializer=tf.zeros_initializer, trainable=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        filt_stride = [1, self.filt_stride[0], self.filt_stride[1], 1]\n",
    "        out_layer = tf.nn.conv2d(inputs, self.w, filt_stride, padding='SAME')\n",
    "        # add the bias\n",
    "        out_layer += self.b\n",
    "        out_layer = self.activation(out_layer)\n",
    "        pool_shape = [1, self.pool_size[0], self.pool_size[1], 1]\n",
    "        pool_strides = [1, self.pool_stride[0], self.pool_stride[1], 1]\n",
    "        out_layer = tf.nn.max_pool(out_layer, ksize=pool_shape, strides=pool_strides, padding='SAME')\n",
    "        return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* O primeiro argumento é a função de ativação\n",
    "* O segundo é o numero de canais de entrada (input channel) para a primeira camada convolucional, que é 1 pois corresponde ao único canal de uma imagem em escala de cinza\n",
    "* O terceiro é o número de canais de saída, que será 32. Ou seja, terão 32 janelas deslizantes (convoluções)\n",
    "* Para o segundo Layer a entrada serão os 32 canais gerados no primero e a saída sera 64 canais\n",
    "* O quarto é o tamanho da janela usada para a convolução, nesse caso será uma janela de 5x5\n",
    "* o quinto é o tamanho da janela do pooling que é o responsavél por diminuir features e generalizar mais o modelo (2x2)\n",
    "* o sexto e o sétimo são strides para a camada de convolução e pooling, são eles que dizem como a janela deslizante irá se movimentar.\n",
    "* o último é um inicializador de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    ConvLayer(tf.nn.relu, 1, 32, [5, 5], [2, 2], [1, 1], [2, 2]),\n",
    "    ConvLayer(tf.nn.relu, 32, 64, [5, 5], [2, 2], [1, 1], [2, 2]),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(300, activation=tf.nn.relu, kernel_initializer=tf.keras.initializers.he_normal()),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10, activation=None)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, size):\n",
    "    idxs = np.random.randint(0, len(y), size)\n",
    "    return x[idxs,:,:], y[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, labels):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = labels, \n",
    "                                                                           logits= logits))\n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, loss=0.170, train accuracy=96.875%, test accuracy=98.820%\n",
      "Iter: 50, loss=0.091, train accuracy=96.875%, test accuracy=98.640%\n",
      "Iter: 100, loss=0.001, train accuracy=100.000%, test accuracy=98.750%\n",
      "Iter: 150, loss=0.010, train accuracy=100.000%, test accuracy=97.870%\n",
      "Iter: 200, loss=0.013, train accuracy=100.000%, test accuracy=98.420%\n",
      "Iter: 250, loss=0.625, train accuracy=90.625%, test accuracy=98.030%\n",
      "Iter: 300, loss=0.160, train accuracy=96.875%, test accuracy=98.570%\n",
      "Iter: 350, loss=0.213, train accuracy=96.875%, test accuracy=98.700%\n",
      "Iter: 400, loss=0.062, train accuracy=96.875%, test accuracy=98.740%\n",
      "Iter: 450, loss=0.010, train accuracy=100.000%, test accuracy=98.240%\n",
      "Iter: 500, loss=0.023, train accuracy=100.000%, test accuracy=98.900%\n",
      "Iter: 550, loss=0.037, train accuracy=96.875%, test accuracy=98.690%\n",
      "Iter: 600, loss=0.054, train accuracy=96.875%, test accuracy=98.570%\n",
      "Iter: 650, loss=0.118, train accuracy=96.875%, test accuracy=98.680%\n",
      "Iter: 700, loss=0.075, train accuracy=96.875%, test accuracy=98.620%\n",
      "Iter: 750, loss=0.137, train accuracy=96.875%, test accuracy=98.750%\n",
      "Iter: 800, loss=0.001, train accuracy=100.000%, test accuracy=98.960%\n",
      "Iter: 850, loss=0.001, train accuracy=100.000%, test accuracy=98.900%\n",
      "Iter: 900, loss=0.008, train accuracy=100.000%, test accuracy=99.060%\n",
      "Iter: 950, loss=0.041, train accuracy=96.875%, test accuracy=98.970%\n",
      "Final test accuracy is 98.30%\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "iterations = 1000\n",
    "batch_size = 32\n",
    "train_writer = tf.summary.create_file_writer(\"tf_vis/cnn_mnist\")\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "x_test = tf.Variable(x_test)\n",
    "x_test = tf.cast(x_test, tf.float32)\n",
    "x_test = tf.reshape(x_test, (len(x_test),28, 28, 1))\n",
    "\n",
    "for i in range(iterations):\n",
    "    batch_x, batch_y = get_batch(x_train, y_train, batch_size)\n",
    "    \n",
    "    # create tensors\n",
    "    batch_x = tf.Variable(batch_x)\n",
    "    batch_y = tf.Variable(batch_y)\n",
    "    batch_y = tf.cast(batch_y, tf.int32)\n",
    "    batch_y_one_hot = tf.one_hot(batch_y, 10)\n",
    "    \n",
    "    # get the images in the right format\n",
    "    batch_x = tf.cast(batch_x, tf.float32)\n",
    "    batch_x = tf.reshape(batch_x, (batch_size, 28, 28, 1))\n",
    "            \n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(batch_x)\n",
    "        loss = loss_fn(logits, batch_y_one_hot)\n",
    "        \n",
    "        \n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        max_idxs = tf.argmax(logits, axis=1)\n",
    "        \n",
    "        train_acc = np.sum(max_idxs.numpy() == batch_y.numpy()) / len(batch_y)\n",
    "        test_logits = model(x_test, training=False)\n",
    "        max_idxs = tf.argmax(test_logits, axis=1)\n",
    "        test_acc = np.sum(max_idxs.numpy() == y_test) / len(y_test)\n",
    "        print(f\"Iter: {i}, loss={loss:.3f}, train accuracy={train_acc * 100:.3f}%, test accuracy={test_acc * 100:.3f}%\")\n",
    "        with train_writer.as_default():\n",
    "            tf.summary.scalar('loss', loss, step=i)\n",
    "            tf.summary.scalar('train_accuracy', train_acc, step=i)\n",
    "            tf.summary.scalar('test_accuracy', test_acc, step=i)\n",
    "# determine the test accuracy\n",
    "logits = model(x_test, training=False)\n",
    "max_idxs = tf.argmax(logits, axis=1)\n",
    "acc = np.sum(max_idxs.numpy() == y_test) / len(y_test)\n",
    "print(\"Final test accuracy is {:.2f}%\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4",
   "language": "python",
   "name": "other-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
